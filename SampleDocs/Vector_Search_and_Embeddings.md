# ベクトル検索と埋め込み（Embeddings）の基礎

## 1. 埋め込み（Embeddings）とは何か
自然言語処理（NLP）において、テキストデータ（単語、文章、パラグラフなど）を固定長の数値ベクトル（数百〜数千次元の実数配列）に変換する技術を「埋め込み（Embeddings）」と呼びます。
テキストをベクトル空間上に配置することで、意味的に近い言葉や文章が空間上で近くに位置するようになり、コンピュータが「意味の類似性」を数学的に計算できるようになります。

### 代表的な埋め込みモデル
* **Word2Vec / GloVe:** 単語レベルの埋め込み。文脈に依存しない静的なベクトルを生成する。
* **BERT:** Transformerアーキテクチャを用いたモデル。前後の文脈を考慮した動的な埋め込みを生成する。
* **OpenAI text-embedding-3 / BGE-M3:** 現代の大規模言語モデル（LLM）で利用される、高度な文章レベルの埋め込みモデル。多言語に対応し、高い精度を持つ。

## 2. ベクトル空間での距離計算
文字列同士の完全一致検索とは異なり、ベクトル検索では「意味の近さ」を距離や角度を用いて計算します。特によく使われる指標が**コサイン類似度（Cosine Similarity）**です。

* **コサイン類似度:** 2つのベクトルが成す角度のコサイン（余弦）を計算します。値は-1から1の間をとり、1に近いほどベクトルの向きが近く、意味が似ていることを示します。
* **ユークリッド距離（L2距離）:** 空間上の2点間の直線距離。ノルムが正規化されている場合、コサイン類似度と等価な結果をもたらします。
* **内積（Dot Product）:** 個々の次元の積の和。正規化されていないベクトルでは大きさが影響しますが、計算が高速です。

## 3. ベクトルデータベースとその役割
数十万、数百万という膨大な数のベクトルから、クエリベクトルに最も近いk個のベクトルを高速に探し出す（近似最近傍探索: ANN）ために特化したデータベースがベクトルデータベースです。

### 代表的なアルゴリズム
* **HNSW (Hierarchical Navigable Small World):** 階層的なグラフ構造を用いて、近傍のベクトルを高速に探索するアルゴリズム。検索精度と速度のバランスに優れ、多くのデータベースで採用されています。
* **IVF-PQ (Inverted File with Product Quantization):** ベクトル空間をクラスタリングし、検索対象を絞り込むとともに、空間を量子化してメモリ使用量を抑える手法。

### 主要なベクトルデータベース製品
* **Milvus / Qdrant / Weaviate:** スケーラビリティに優れたオープンソースベースのベクトルデータベース。
* **Chroma / FAISS:** ローカルでの実験や小規模アプリケーションに組み込みやすい軽量なライブラリ/データベース。

## 4. 従来のキーワード検索（BM25）との比較
ベクトル検索（セマンティック検索）は、同義語や表記揺れに強く、文脈を理解した検索が可能です（例：「車」で検索して「自動車」や「Car」を含む文書を見つける）。
一方で、特定の型番や専門用語など、語彙そのものの一致が重要な場合は、従来のTF-IDFやBM25（Best Matching 25）を用いたキーワード検索の方が優れている場合があります。
そのため、近年では両者を組み合わせた**ハイブリッド検索**が主流になりつつあります。

## 関連ドキュメント
* [[Knowledge_Graph_and_RAG]]
* [[Spreading_Activation_Model]]
